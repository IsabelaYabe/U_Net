{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mElxf7mcM0_Z"
      },
      "source": [
        "\n",
        "# **Programming Assignment 1 - Semantic Segmentation**\n",
        "\n",
        "#### **Professor**: Dário Oliveira  \n",
        "#### **Monitor**: João Alcindo\n",
        "\n",
        "Neste **Programming Assignment (PA)**, você irá trabalhar com a tarefa de **segmentação semântica**, utilizando uma das arquiteturas vistas em aula: a **U-Net**. O objetivo principal é aplicar a rede para segmentação de imagens e ajustar parâmetros para obter melhores resultados. O dataset que deverá ser utilizado pode ser encontrado no seguinte link: [DATASET](https://drive.google.com/file/d/1WUX4z6c7ayJz-NwChkYiybh7WSEpEoDH/view?usp=sharing).\n",
        "\n",
        "\n",
        "![](https://drive.google.com/uc?export=view&id=1JNBM514DlNycUBwH08PSlqKUqOwhk2Qv)\n",
        "\n",
        "\n",
        "\n",
        "### **Instruções:**\n",
        "\n",
        "1. **Escolha do Ambiente de Execução**:  \n",
        "   Você pode optar por utilizar o **Google Colab** ou o **Kaggle Notebook** como ambiente de desenvolvimento. Certifique-se de configurar o ambiente adequadamente, instalando as bibliotecas necessárias.\n",
        "\n",
        "2. **Download dos Dados**:  \n",
        "   Baixe o dataset fornecido e faça a organização adequada das pastas e arquivos para facilitar o carregamento durante a execução do código.\n",
        "\n",
        "3. **Criação de um Dataset Customizado**:  \n",
        "   As máscaras presentes no dataset estão no formato **RGB**. Você precisará convertê-las para uma matriz de **labels**. Existe um arquivo CSV disponível contendo as informações sobre a correspondência entre cores e rótulos. Utilize essas informações para realizar a conversão corretamente.\n",
        "\n",
        "4. **Construção da Arquitetura U-Net**:  \n",
        "   Implemente a U-Net para a tarefa de segmentação. Você pode usar a estrutura básica apresentada em aula ou fazer modificações.\n",
        "\n",
        "   ![Estrutura U-Net](https://camo.githubusercontent.com/6b548ee09b97874014d72903c891360beb0989e74b4585249436421558faa89d/68747470733a2f2f692e696d6775722e636f6d2f6a6544567071462e706e67)\n",
        "\n",
        "5. **Experimentação com Diferentes U-Nets**:  \n",
        "   A U-Net pode ter diferentes capacidades dependendo da sua profundidade. Na imagem acima, o encoder tem cinco blocos convolucionais até o bottleneck (e como ela é espelhada, o decoder também). Experimente variar a profundidade da rede (ou seja, o numero de blocos convolucionais), com 3 blocos e 7 blocos, por exemplo. Qual é o impacto no resultado? Reflita sobre como isso se relaciona com a teoria vista em sala de aula. Analise o impacto no desempenho e na quantidade de parâmetros.\n",
        "\n",
        "6. **Summary da Arquitetura**:  \n",
        "   Utilize a função `summary()` para exibir a quantidade de parâmetros do seu modelo.\n",
        "\n",
        "7. **Escolha do Otimizador e Função de Perda**:  \n",
        "   Escolha um otimizador e a função de perda apropriada para a tarefa de segmentação semântica.\n",
        "\n",
        "8. **Função de Treinamento**:  \n",
        "   Crie uma função de treinamento. Salve as métricas durante o treinamento em um dicionário `history`, contendo os valores de `train_loss`, `val_loss`, `train_acc` e `val_acc` a cada época.\n",
        "\n",
        "9. **Treinamento do Modelo**:  \n",
        "   Treine o modelo, a cada 5 ou 10 epochs, faça o **plot** de uma imagem contendo a **imagem original**, a **máscara verdadeira (ground truth)** e a **máscara predita pelo modelo**. Isso ajudará a visualizar a evolução da segmentação.\n",
        "\n",
        "10. **Data Augmentation**:  \n",
        "    Implemente um procedimento de data augmentation e treine novamente a melhor configuração observada. Houve ganho no desempenho? Reflita sobre os motivos.\n",
        "\n",
        "\n",
        "11. **Preparação de uma Apresentação**:  \n",
        "    Ao final, prepare uma apresentação resumindo os passos seguidos, resultados obtidos, gráficos de perdas e acurácia, e discussões sobre o desempenho do modelo. Lembre de fundamentar a discussão com os aspectos teoricos vistos em sala de aula.\n",
        "\n",
        "\n",
        "### **Pontos Importantes:**\n",
        "\n",
        "- Escolher adequadamente o tamanho do BATCH, Loss Function e Otimizador e saber o motivo de cada escolha;\n",
        "- Monitore o uso das GPUs, o kaggle te informa quantidade de tempo disponível, mas o colab não;\n",
        "- Observe as classes com mais erros por parte do modelo;\n",
        "- Adicione gráficos de perda e acurácia na sua apresentação;\n",
        "- Coloque imagens das predições do modelo;\n",
        "- Use **Pytorch !!!**.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {},
      "outputs": [],
      "source": [
        "import os\n",
        "import cv2\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "from PIL import Image\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import random\n",
        "import torch\n",
        "import shutil"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "42SkAf6SSR8V"
      },
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": 60,
      "metadata": {},
      "outputs": [],
      "source": [
        "class PreprocessDataset(Dataset):\n",
        "    def __init__(self, dir_dataset, df_labels, dir_new_dataset, set_type=\"train\", with_brightness_contrast=True, var_gaussian=10, amount = 0.02, prob_gen=0.4, partition=3): \n",
        "        self.with_brightness_contrast = with_brightness_contrast\n",
        "        self.var_gaussian = var_gaussian\n",
        "        self.amount = amount\n",
        "        self.dir_dataset = dir_dataset\n",
        "        self.prob_gen = prob_gen\n",
        "        self.set_type = set_type\n",
        "        self.dir_new_dataset = dir_new_dataset\n",
        "        self.partition = partition\n",
        "        self.rgb_to_index, self.index_to_label = self.dict_labels(df_labels)\n",
        "        self.df_labels = df_labels\n",
        "        self.dim_images = None\n",
        "        self.num_images = None\n",
        "        self.image_list = [] \n",
        "        self.label_list = [] \n",
        "        self.num_classes = len(self.df_labels)\n",
        "        self.load_images_from_folder()\n",
        "    def __len__(self):\n",
        "        return len(self.image_list)\n",
        "    \n",
        "    def __getitem__(self, idx):\n",
        "        # Carregar a imagem e o rótulo baseado no índice `idx`\n",
        "        img_path = self.image_list[idx]\n",
        "        lbl_path = self.label_list[idx]\n",
        "\n",
        "        # Carregar a imagem e o rótulo do disco\n",
        "        img = cv2.imread(img_path)\n",
        "        lbl = cv2.imread(lbl_path)\n",
        "\n",
        "        # Normalizar a imagem (opcional)\n",
        "        img = img / 255.0\n",
        "\n",
        "        # Converter os dados para tensores\n",
        "        img = torch.tensor(img, dtype=torch.float32).permute(2, 0, 1)  # De (H, W, C) para (C, H, W)\n",
        "        lbl = torch.tensor(lbl, dtype=torch.long)\n",
        "\n",
        "        return img, lbl\n",
        "\n",
        "    def create_or_reset_directory(self):\n",
        "        new_dir_img = os.path.join(self.dir_new_dataset, self.set_type)\n",
        "        new_dir_lbl = os.path.join(self.dir_new_dataset, f\"{self.set_type}_labels\")\n",
        "        \n",
        "        # Verificar se o diretório de imagens já existe; se não, criar\n",
        "        if not os.path.exists(new_dir_img):\n",
        "            os.makedirs(new_dir_img)  # Cria o diretório se não existir\n",
        "        \n",
        "        # Verificar se o diretório de rótulos já existe; se não, criar\n",
        "        if not os.path.exists(new_dir_lbl):\n",
        "            os.makedirs(new_dir_lbl)  # Cria o diretório se não existir\n",
        "        \n",
        "    def load_images_from_folder(self):\n",
        "        # Definir os caminhos originais dos datasets\n",
        "        images_dir = os.path.join(self.dir_dataset, self.set_type)\n",
        "        labels_dir = os.path.join(self.dir_dataset, f\"{self.set_type}_labels\")\n",
        "        \n",
        "        dim_images = self.load_images_and_labels(images_dir, labels_dir)\n",
        "        self.dim_images = dim_images\n",
        "            \n",
        "    def load_images_and_labels(self, images_dir, labels_dir):\n",
        "        self.create_or_reset_directory()\n",
        "\n",
        "        # Listar arquivos nos diretórios\n",
        "        image_files = sorted(os.listdir(images_dir))\n",
        "        label_files = sorted(os.listdir(labels_dir))\n",
        "\n",
        "        prob_to_transform = self.prob_gen\n",
        "        img_lbl_files = zip(image_files, label_files)\n",
        "\n",
        "        for img_name, lbl_name in img_lbl_files:\n",
        "            transform_img = random.choices([True, False], weights=[prob_to_transform, 1 - prob_to_transform], k=1)[0]\n",
        "\n",
        "            # Definir caminho para a imagem e rótulo\n",
        "            img_path = os.path.join(images_dir, img_name)\n",
        "            lbl_path = os.path.join(labels_dir, lbl_name)\n",
        "\n",
        "            # Carregar imagem e rótulo\n",
        "            img = cv2.imread(img_path)\n",
        "            lbl = cv2.imread(lbl_path)\n",
        "\n",
        "            # Reduzir a resolução da imagem\n",
        "            img = img[::self.partition, ::self.partition, :]\n",
        "            lbl = lbl[::self.partition, ::self.partition, :]\n",
        "\n",
        "            # Converter de BGR para RGB\n",
        "            img = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)\n",
        "            lbl = cv2.cvtColor(lbl, cv2.COLOR_BGR2RGB)\n",
        "\n",
        "            # Aplicar Gaussian blur\n",
        "            img = cv2.GaussianBlur(img, (3, 3), 0)\n",
        "\n",
        "            # Transformação aleatória\n",
        "            if transform_img:\n",
        "                img_transform = img.copy()\n",
        "                lbl_transform = lbl.copy()\n",
        "                img_transform, lbl_transform, trans = self.apply_random_transformation(img_transform, lbl_transform)\n",
        "\n",
        "                # Salvar imagem e rótulo transformado com sufixo de transformação\n",
        "                transformed_img_name = f\"{img_name}_{trans}.png\"\n",
        "                transformed_lbl_name = f\"{lbl_name}_{trans}.png\"\n",
        "\n",
        "                processed_img_path = os.path.join(self.dir_new_dataset, self.set_type, transformed_img_name)\n",
        "                processed_lbl_path = os.path.join(self.dir_new_dataset, f\"{self.set_type}_labels\", transformed_lbl_name)\n",
        "\n",
        "                # Verifique se o arquivo já existe antes de salvar\n",
        "                if not os.path.exists(processed_img_path):\n",
        "                    cv2.imwrite(processed_img_path, img_transform)\n",
        "                    cv2.imwrite(processed_lbl_path, lbl_transform)\n",
        "\n",
        "                    # Adicionar as imagens e rótulos transformados às listas\n",
        "                    self.image_list.append(processed_img_path)\n",
        "                    self.label_list.append(processed_lbl_path)\n",
        "\n",
        "            # Criar caminho para salvar a imagem transformada\n",
        "            processed_img_path = os.path.join(self.dir_new_dataset, self.set_type, img_name)\n",
        "            processed_lbl_path = os.path.join(self.dir_new_dataset, f\"{self.set_type}_labels\", lbl_name)\n",
        "\n",
        "            cv2.imwrite(processed_img_path, img)\n",
        "            cv2.imwrite(processed_lbl_path, lbl)\n",
        "            \n",
        "            # Adicionar as imagens e rótulos processados às listas\n",
        "            self.image_list.append(processed_img_path)\n",
        "            self.label_list.append(processed_lbl_path)\n",
        "\n",
        "        return img.shape\n",
        "\n",
        "    def apply_random_transformation(self, img, lbl):\n",
        "        if self.with_brightness_contrast:\n",
        "            random_transform = random.randint(0, 9)\n",
        "        else:\n",
        "            random_transform = random.randint(0, 4)\n",
        "        if random_transform == 0:\n",
        "            arg_1, arg_2 = self.augment(img, lbl, \"rotation\")\n",
        "            return arg_1, arg_2, \"rotation\"\n",
        "        elif random_transform == 1:\n",
        "            arg_1, arg_2 = self.augment(img, lbl, \"flip\")\n",
        "            return arg_1, arg_2, \"flip\"\n",
        "        elif random_transform == 2:\n",
        "            arg_1, arg_2 = self.augment(img, lbl, \"flip_rotation\")\n",
        "            return arg_1, arg_2, \"flip_rotation\"\n",
        "        elif random_transform == 3:\n",
        "            arg_1, arg_2 = self.equalize_histogram(img, lbl)\n",
        "            return arg_1, arg_2, \"equalize_histogram\"\n",
        "        elif random_transform == 4:\n",
        "            arg_1, arg_2 = self.add_noise(img, lbl, noise_type=\"gaussian\")\n",
        "            return arg_1, arg_2, \"gaussian_noise\"\n",
        "        elif random_transform == 5:\n",
        "            arg_1, arg_2 = self.add_noise(img, lbl, noise_type=\"salt_pepper\")\n",
        "            return arg_1, arg_2, \"sal_pepper_noise\"\n",
        "        elif random_transform == 6:\n",
        "            arg_1, arg_2 = self.adjust_brightness_contrast(img, lbl, brightness=0, contrast=50)\n",
        "            return arg_1, arg_2, \"b_c_0_50\"\n",
        "        elif random_transform == 7:\n",
        "            arg_1, arg_2 = self.adjust_brightness_contrast(img, lbl, brightness=-10, contrast=30)\n",
        "            return arg_1, arg_2, \"b_c_-10_30\"\n",
        "        elif random_transform == 8:\n",
        "            arg_1, arg_2 = self.adjust_brightness_contrast(img, lbl, brightness=60, contrast=60)\n",
        "            return arg_1, arg_2, \"b_c_60_60\"\n",
        "        else:\n",
        "            arg_1, arg_2 = self.adjust_brightness_contrast(img, lbl, brightness=-20, contrast=0)\n",
        "            return arg_1, arg_2, \"b_c_-20_0\"\n",
        "\n",
        "    def rgb_to_label(self, mask):\n",
        "        unique_colors = np.unique(mask.reshape(-1, mask.shape[2]), axis=0)\n",
        "        label_mask = np.zeros((mask.shape[0], mask.shape[1]), dtype=np.int32)\n",
        "        for color in unique_colors:\n",
        "            color_tuple = tuple(color)\n",
        "            if color_tuple in self.rgb_to_index:\n",
        "                label = self.rgb_to_index[color_tuple]\n",
        "                matches = np.all(mask == color, axis=-1)\n",
        "                label_mask[matches] = label\n",
        "        return label_mask\n",
        "\n",
        "    def dict_labels(self, df_labels):\n",
        "        rgb_to_index = {}\n",
        "        index_to_label = {}\n",
        "        for count, row in df_labels.iterrows():\n",
        "            color = (row['r'], row['g'], row['b'])\n",
        "            label = row['name']\n",
        "            rgb_to_index[color] = count\n",
        "            index_to_label[count] = label\n",
        "        return rgb_to_index, index_to_label\n",
        "\n",
        "    def augment(self, img, lbl, transform):\n",
        "        if transform == \"rotation\":\n",
        "            rotation = cv2.getRotationMatrix2D((img.shape[1] // 2, img.shape[0] // 2), 180, 1)\n",
        "            rotated_image = cv2.warpAffine(img, rotation, (img.shape[1], img.shape[0]))\n",
        "            rotated_label = cv2.warpAffine(lbl, rotation, (lbl.shape[1], lbl.shape[0]), flags=cv2.INTER_NEAREST)\n",
        "            return rotated_image, rotated_label\n",
        "        elif transform == \"flip\":\n",
        "            flipped_image = cv2.flip(img, 1)\n",
        "            flipped_label = cv2.flip(lbl, 1)\n",
        "            return flipped_image, flipped_label\n",
        "        elif transform == \"flip_rotation\":\n",
        "            flipped_image = cv2.flip(img, 0)\n",
        "            flipped_label = cv2.flip(lbl, 0)\n",
        "            return flipped_image, flipped_label\n",
        "        \n",
        "   \n",
        "    def equalize_histogram(self, img, lbl):\n",
        "        img_yuv = cv2.cvtColor(img, cv2.COLOR_BGR2YUV)\n",
        "        img_yuv[:, :, 0] = cv2.equalizeHist(img_yuv[:, :, 0])\n",
        "        img = cv2.cvtColor(img_yuv, cv2.COLOR_YUV2BGR)\n",
        "        return img, lbl\n",
        "        \n",
        "    def adjust_brightness_contrast(self, img, lbl, brightness=0, contrast=0):\n",
        "        img = np.int16(img)\n",
        "        img = img * (contrast / 127 + 1) - contrast + brightness\n",
        "        img = np.clip(img, 0, 255)\n",
        "        img = np.uint8(img)\n",
        "        return img, lbl\n",
        "    \n",
        "    def add_noise(self, img, lbl, noise_type=\"gaussian\"):\n",
        "        if noise_type == \"gaussian\":\n",
        "            mean = 0\n",
        "            var = self.var_gaussian\n",
        "            sigma = var ** 0.5\n",
        "            gauss = np.random.normal(mean, sigma, img.shape)\n",
        "            noisy = img + gauss\n",
        "            noisy = np.clip(noisy, 0, 255)\n",
        "            return noisy, lbl\n",
        "    \n",
        "        elif noise_type == \"salt_pepper\":\n",
        "            s_vs_p = 0.5\n",
        "            amount = self.amount\n",
        "            out = np.copy(img)\n",
        "    \n",
        "            # Salt mode\n",
        "            num_salt = np.ceil(amount * img.size * s_vs_p)\n",
        "            coords = [np.random.randint(0, i, int(num_salt)) for i in img.shape]\n",
        "            out[tuple(coords)] = 255\n",
        "    \n",
        "            # Pepper mode\n",
        "            num_pepper = np.ceil(amount * img.size * (1.0 - s_vs_p))\n",
        "            coords = [np.random.randint(0, i, int(num_pepper)) for i in img.shape]\n",
        "            out[tuple(coords)] = 0\n",
        "    \n",
        "            return out, lbl"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 66,
      "metadata": {},
      "outputs": [],
      "source": [
        "class UNet(nn.Module):\n",
        "    def __init__(self, image_dim, n_channels=3, n_classes=32, depth=5, conv_kernel_size=3, conv_stride=1, conv_padding=1, pool_kernel_size=2, pool_stride=2, pool_padding=0, transpose_kernel_size=3, transpose_stride=2, transpose_padding=1):\n",
        "        super(UNet, self).__init__()\n",
        "\n",
        "        self.image_dim = image_dim  # Dimensões da imagem de entrada (C, H, W)\n",
        "        self.depth = depth\n",
        "        self.n_channels = n_channels\n",
        "        self.n_classes = n_classes\n",
        "        self.conv_kernel_size = conv_kernel_size\n",
        "        self.conv_stride = conv_stride\n",
        "        self.conv_padding = conv_padding\n",
        "        self.pool_kernel_size = pool_kernel_size\n",
        "        self.pool_stride = pool_stride\n",
        "        self.pool_padding = pool_padding\n",
        "        self.transpose_kernel_size = transpose_kernel_size\n",
        "        self.transpose_stride = transpose_stride\n",
        "        self.transpose_padding = transpose_padding\n",
        "\n",
        "        # Encoder\n",
        "        self.encoders = nn.ModuleList([self.conv_block(self.n_channels if i == 0 else 64 * (2 ** (i-1)), 64 * (2 ** i)) for i in range(depth)])\n",
        "        self.pool = nn.MaxPool2d(kernel_size=self.pool_kernel_size, stride=self.pool_stride, padding=self.pool_padding)\n",
        "\n",
        "        # Bottleneck\n",
        "        self.bottleneck = self.conv_block(64 * (2 ** (depth-1)), 64 * (2 ** depth))\n",
        "\n",
        "        # Decoder\n",
        "        self.decoders = nn.ModuleList([self.conv_transpose(64 * (2 ** (i+1)), 64 * (2 ** i)) for i in range(depth-1, -1, -1)])\n",
        "\n",
        "        # Final conv layer\n",
        "        self.final_conv = nn.Conv2d(64, n_classes, kernel_size=1)\n",
        "\n",
        "    def conv_block(self, in_channels, out_channels):\n",
        "        # Camada convolucional com normalização e função de ativação; 2 vezes\n",
        "        return nn.Sequential(\n",
        "            nn.Conv2d(in_channels, out_channels, kernel_size=self.conv_kernel_size, stride=self.conv_stride, padding=self.conv_padding),\n",
        "            nn.BatchNorm2d(out_channels),  # Normalização para acelerar o treinamento\n",
        "            nn.ReLU(inplace=True),  # Função de ativação (zera os valores negativos)\n",
        "            nn.Conv2d(out_channels, out_channels, kernel_size=self.conv_kernel_size, stride=self.conv_stride, padding=self.conv_padding),\n",
        "            nn.BatchNorm2d(out_channels),\n",
        "            nn.ReLU(inplace=True)\n",
        "        )\n",
        "\n",
        "    def crop(self, encoder_feature, decoder_feature):\n",
        "        _, _, h, w = decoder_feature.size()\n",
        "        encoder_feature = F.interpolate(encoder_feature, size=(h, w), mode='bilinear', align_corners=False)  # Redimensiona a feature map do encoder\n",
        "        return encoder_feature\n",
        "\n",
        "    def conv_transpose(self, in_channels, out_channels):\n",
        "        return nn.Sequential(\n",
        "            nn.ConvTranspose2d(in_channels, out_channels, kernel_size=self.transpose_kernel_size, stride=self.transpose_stride, padding=self.transpose_padding),\n",
        "            self.conv_block(out_channels, out_channels)\n",
        "        )\n",
        "    \n",
        "    def forward(self, x):\n",
        "        print(f\"Input shape: {x.shape}\")\n",
        "        encoders_features = []\n",
        "\n",
        "        # Encoder pass\n",
        "        for idx, encoder in enumerate(self.encoders):\n",
        "            x = encoder(x)\n",
        "            encoders_features.append(x)\n",
        "            print(f\"After encoder block {idx+1}: {x.shape}\")\n",
        "            x = self.pool(x)\n",
        "            print(f\"After pooling {idx+1}: {x.shape}\")\n",
        "\n",
        "        # Bottleneck\n",
        "        x = self.bottleneck(x)\n",
        "        print(f\"After bottleneck: {x.shape}\")\n",
        "\n",
        "        # Decoder pass\n",
        "        for i, decoder in enumerate(self.decoders):\n",
        "            encoder_feature = encoders_features[-(i+1)]\n",
        "            encoder_feature = self.crop(encoder_feature, x)  # Aplica o crop nas feature maps\n",
        "            print(f\"Encoder feature {i+1} after crop: {encoder_feature.shape}\")\n",
        "\n",
        "            x = torch.cat([encoder_feature, x], dim=1)  # Concatena encoder com decoder\n",
        "            print(f\"After concatenation with encoder feature {i+1}: {x.shape}\")\n",
        "\n",
        "            x = decoder(x)\n",
        "            print(f\"After decoder block {i+1}: {x.shape}\")\n",
        "\n",
        "        # Final convolution\n",
        "        x = self.final_conv(x)\n",
        "        print(f\"Output shape after final convolution: {x.shape}\")\n",
        "\n",
        "        return x"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 67,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Cuda is available:  True\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "<>:4: SyntaxWarning: invalid escape sequence '\\C'\n",
            "<>:4: SyntaxWarning: invalid escape sequence '\\C'\n",
            "C:\\Users\\Isas_\\AppData\\Local\\Temp\\ipykernel_9620\\1077995894.py:4: SyntaxWarning: invalid escape sequence '\\C'\n",
            "  dir_dataset = \"data\\CamVid\"\n"
          ]
        }
      ],
      "source": [
        "print(\"Cuda is available: \", torch.cuda.is_available())\n",
        "\n",
        "# Definindo os caminhos para cada conjunto\n",
        "dir_dataset = \"data\\CamVid\"\n",
        "\n",
        "class_dict = os.path.join(dir_dataset, 'class_dict.csv')\n",
        "df_labels = pd.read_csv(class_dict)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 68,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Função para remover todos subdiretórios e arquivos de um diretório\n",
        "def remove_all_files_from_dir(dir_path):\n",
        "    for item in os.listdir(dir_path):\n",
        "        item_path = os.path.join(dir_path, item)\n",
        "        if os.path.isfile(item_path):\n",
        "            os.remove(item_path)\n",
        "        elif os.path.isdir(item_path):\n",
        "            shutil.rmtree(item_path)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 71,
      "metadata": {},
      "outputs": [],
      "source": [
        "remove_all_files_from_dir(\"processed_data\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 72,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Batch de imagens: torch.Size([16, 3, 180, 240])\n",
            "Batch de labels: torch.Size([16, 180, 240, 3])\n"
          ]
        }
      ],
      "source": [
        "\n",
        "\n",
        "# Parâmetros\n",
        "partition = 4  # Reduzir o tamanho da imagem por um fator de 4\n",
        "prob_train = 0.25  # Probabilidade de aplicar transformações no conjunto de treino\n",
        "prob_others = 0.10  # Probabilidade de aplicar transformações nos conjuntos de validação e teste\n",
        "dir_new_dataset = \"processed_data\"  # Diretório onde os conjuntos processados serão salvos\n",
        "with_brightness_contrast = True  # Aplicar transformações de brilho e contraste\n",
        "var_gaussian = 20  # Variância do ruído gaussiano\n",
        "amount = 0.02  # Quantidade de ruído sal e pimenta\n",
        "\n",
        "# Criando o dataset para o conjunto de treino\n",
        "train_dataset = PreprocessDataset(\n",
        "    dir_dataset=dir_dataset,\n",
        "    df_labels=df_labels,\n",
        "    dir_new_dataset=dir_new_dataset,\n",
        "    set_type='train',  # Carrega o conjunto de treino\n",
        "    partition=partition  # Reduzir a imagem por um fator de 4\n",
        ")\n",
        "\n",
        "# Criando o dataset para o conjunto de validação\n",
        "val_dataset = PreprocessDataset(\n",
        "    dir_dataset=dir_dataset,\n",
        "    df_labels=df_labels,\n",
        "    dir_new_dataset=dir_new_dataset,\n",
        "    set_type='val',  # Carrega o conjunto de validação\n",
        "    partition=partition\n",
        ")\n",
        "\n",
        "# Criando o dataset para o conjunto de teste\n",
        "test_dataset = PreprocessDataset(\n",
        "    dir_dataset=dir_dataset,\n",
        "    df_labels=df_labels,\n",
        "    dir_new_dataset=dir_new_dataset,\n",
        "    set_type='test',  # Carrega o conjunto de teste\n",
        "    partition=partition\n",
        ")\n",
        "\n",
        "# Definindo o tamanho do batch\n",
        "batch_size = 16\n",
        "\n",
        "# Criando o DataLoader para o conjunto de treino\n",
        "train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
        "\n",
        "# Criando o DataLoader para o conjunto de validação\n",
        "val_loader = DataLoader(val_dataset, batch_size=batch_size, shuffle=False)\n",
        "\n",
        "# Criando o DataLoader para o conjunto de teste\n",
        "test_loader = DataLoader(test_dataset, batch_size=batch_size, shuffle=False)\n",
        "\n",
        "# Iterando sobre o DataLoader de treino\n",
        "for images, labels in train_loader:\n",
        "    print(f'Batch de imagens: {images.shape}')\n",
        "    print(f'Batch de labels: {labels.shape}')\n",
        "    # Aqui você pode passar as imagens e labels para o seu modelo\n",
        "    break  # Apenas um exemplo, interrompendo após o primeiro batch"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Número de batches:  207\n",
            "<torch.utils.data.dataloader.DataLoader object at 0x0000020929F8E270>\n"
          ]
        }
      ],
      "source": [
        "print(\"Número de batches: \", len(train_loader))\n",
        "print(train_loader)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Dispositivo: cuda\n"
          ]
        }
      ],
      "source": [
        "# Instanciando o modelo U-Net com as dimensões das imagens\n",
        "model = UNet(image_dim=(3, 240, 320), n_channels=3, n_classes=32)\n",
        "\n",
        "# Configurando o dispositivo (GPU, se disponível)\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "print(f\"Dispositivo: {device}\")\n",
        "model = model.to(device)  # Mover o modelo para o dispositivo\n",
        "\n",
        "# Testando com um batch de dados do DataLoader\n",
        "for images, labels in train_loader:\n",
        "    # Move os dados para o dispositivo (GPU/CPU)\n",
        "    images = images.to(device)\n",
        "    labels = labels.to(device)\n",
        "    print(\"Imagens e rótulos movidos para o dispositivo.\")\n",
        "    print(images.shape)\n",
        "    print(labels.shape)\n",
        "    print(\"Iniciando forward pass...\")\n",
        "    # Passa as imagens pelo modelo U-Net\n",
        "    output = model(images)\n",
        "    \n",
        "    # Exibe as dimensões das imagens, labels e da saída do modelo\n",
        "    print(f\"Imagens: {images.shape}\")\n",
        "    print(f\"Labels: {labels.shape}\")\n",
        "    print(f\"Saída do modelo: {output.shape}\")\n",
        "    \n",
        "    # Quebrar após o primeiro batch, apenas para teste\n",
        "    break"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
