{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mElxf7mcM0_Z"
      },
      "source": [
        "\n",
        "# **Programming Assignment 1 - Semantic Segmentation**\n",
        "\n",
        "#### **Professor**: Dário Oliveira  \n",
        "#### **Monitor**: João Alcindo\n",
        "\n",
        "Neste **Programming Assignment (PA)**, você irá trabalhar com a tarefa de **segmentação semântica**, utilizando uma das arquiteturas vistas em aula: a **U-Net**. O objetivo principal é aplicar a rede para segmentação de imagens e ajustar parâmetros para obter melhores resultados. O dataset que deverá ser utilizado pode ser encontrado no seguinte link: [DATASET](https://drive.google.com/file/d/1WUX4z6c7ayJz-NwChkYiybh7WSEpEoDH/view?usp=sharing).\n",
        "\n",
        "\n",
        "![](https://drive.google.com/uc?export=view&id=1JNBM514DlNycUBwH08PSlqKUqOwhk2Qv)\n",
        "\n",
        "\n",
        "\n",
        "### **Instruções:**\n",
        "\n",
        "1. **Escolha do Ambiente de Execução**:  \n",
        "   Você pode optar por utilizar o **Google Colab** ou o **Kaggle Notebook** como ambiente de desenvolvimento. Certifique-se de configurar o ambiente adequadamente, instalando as bibliotecas necessárias.\n",
        "\n",
        "2. **Download dos Dados**:  \n",
        "   Baixe o dataset fornecido e faça a organização adequada das pastas e arquivos para facilitar o carregamento durante a execução do código.\n",
        "\n",
        "3. **Criação de um Dataset Customizado**:  \n",
        "   As máscaras presentes no dataset estão no formato **RGB**. Você precisará convertê-las para uma matriz de **labels**. Existe um arquivo CSV disponível contendo as informações sobre a correspondência entre cores e rótulos. Utilize essas informações para realizar a conversão corretamente.\n",
        "\n",
        "4. **Construção da Arquitetura U-Net**:  \n",
        "   Implemente a U-Net para a tarefa de segmentação. Você pode usar a estrutura básica apresentada em aula ou fazer modificações.\n",
        "\n",
        "   ![Estrutura U-Net](https://camo.githubusercontent.com/6b548ee09b97874014d72903c891360beb0989e74b4585249436421558faa89d/68747470733a2f2f692e696d6775722e636f6d2f6a6544567071462e706e67)\n",
        "\n",
        "5. **Experimentação com Diferentes U-Nets**:  \n",
        "   A U-Net pode ter diferentes capacidades dependendo da sua profundidade. Na imagem acima, o encoder tem cinco blocos convolucionais até o bottleneck (e como ela é espelhada, o decoder também). Experimente variar a profundidade da rede (ou seja, o numero de blocos convolucionais), com 3 blocos e 7 blocos, por exemplo. Qual é o impacto no resultado? Reflita sobre como isso se relaciona com a teoria vista em sala de aula. Analise o impacto no desempenho e na quantidade de parâmetros.\n",
        "\n",
        "6. **Summary da Arquitetura**:  \n",
        "   Utilize a função `summary()` para exibir a quantidade de parâmetros do seu modelo.\n",
        "\n",
        "7. **Escolha do Otimizador e Função de Perda**:  \n",
        "   Escolha um otimizador e a função de perda apropriada para a tarefa de segmentação semântica.\n",
        "\n",
        "8. **Função de Treinamento**:  \n",
        "   Crie uma função de treinamento. Salve as métricas durante o treinamento em um dicionário `history`, contendo os valores de `train_loss`, `val_loss`, `train_acc` e `val_acc` a cada época.\n",
        "\n",
        "9. **Treinamento do Modelo**:  \n",
        "   Treine o modelo, a cada 5 ou 10 epochs, faça o **plot** de uma imagem contendo a **imagem original**, a **máscara verdadeira (ground truth)** e a **máscara predita pelo modelo**. Isso ajudará a visualizar a evolução da segmentação.\n",
        "\n",
        "10. **Data Augmentation**:  \n",
        "    Implemente um procedimento de data augmentation e treine novamente a melhor configuração observada. Houve ganho no desempenho? Reflita sobre os motivos.\n",
        "\n",
        "\n",
        "11. **Preparação de uma Apresentação**:  \n",
        "    Ao final, prepare uma apresentação resumindo os passos seguidos, resultados obtidos, gráficos de perdas e acurácia, e discussões sobre o desempenho do modelo. Lembre de fundamentar a discussão com os aspectos teoricos vistos em sala de aula.\n",
        "\n",
        "\n",
        "### **Pontos Importantes:**\n",
        "\n",
        "- Escolher adequadamente o tamanho do BATCH, Loss Function e Otimizador e saber o motivo de cada escolha;\n",
        "- Monitore o uso das GPUs, o kaggle te informa quantidade de tempo disponível, mas o colab não;\n",
        "- Observe as classes com mais erros por parte do modelo;\n",
        "- Adicione gráficos de perda e acurácia na sua apresentação;\n",
        "- Coloque imagens das predições do modelo;\n",
        "- Use **Pytorch !!!**.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 68,
      "metadata": {},
      "outputs": [],
      "source": [
        "import os\n",
        "import cv2\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "from PIL import Image\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import random\n",
        "import torch\n",
        "import shutil"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "42SkAf6SSR8V"
      },
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": 147,
      "metadata": {},
      "outputs": [],
      "source": [
        "class PreprocessDataset(Dataset):\n",
        "    def __init__(self, dir_dataset, df_labels, dir_new_dataset, with_brightness_contrast=True, var_gaussian=10, amount = 0.02, prob_train=0.4, prob_others=0.1, partition=3): \n",
        "        self.with_brightness_contrast = with_brightness_contrast\n",
        "        self.var_gaussian = var_gaussian\n",
        "        self.amount = amount\n",
        "        self.dir_dataset = dir_dataset\n",
        "        self.prob_train = prob_train\n",
        "        self.prob_others = prob_others\n",
        "        self.dir_new_dataset = dir_new_dataset\n",
        "        self.partition = partition\n",
        "        self.rgb_to_index, self.index_to_label = self.dict_labels(df_labels)\n",
        "        self.df_labels = df_labels\n",
        "        self.dim_images = None\n",
        "        self.num_images_train = None\n",
        "        self.num_images_val = None\n",
        "        self.num_images_test = None\n",
        "        self.image_list = [] \n",
        "        self.label_list = [] \n",
        "        self.num_classes = len(self.df_labels)\n",
        "\n",
        "    def __len__(self):\n",
        "        return self.num_images_train + self.num_images_val + self.num_images_test\n",
        "    \n",
        "    def __getitem__(self, idx):\n",
        "        # Carregar a imagem e o rótulo baseado no índice `idx`\n",
        "        img_path = self.image_list[idx]\n",
        "        lbl_path = self.label_list[idx]\n",
        "        \n",
        "        # Carregar a imagem e o rótulo do disco\n",
        "        img = cv2.imread(img_path)\n",
        "        lbl = cv2.imread(lbl_path)\n",
        "        \n",
        "        # Normalizar a imagem (opcional)\n",
        "        img = img / 255.0\n",
        "        \n",
        "        # Converter os dados para tensores\n",
        "        img = torch.tensor(img, dtype=torch.float32).permute(2, 0, 1)  # De (H, W, C) para (C, H, W)\n",
        "        lbl = torch.tensor(lbl, dtype=torch.long)\n",
        "        \n",
        "        return img, lbl\n",
        "\n",
        "    def create_or_reset_directory(self, path):\n",
        "        # Se o diretório já existir, esvaziar\n",
        "        if os.path.exists(path):\n",
        "            shutil.rmtree(path)  # Remove todo o conteúdo\n",
        "        os.makedirs(path)  # Cria o diretório novamente\n",
        "        \n",
        "    def load_images_from_folder(self):\n",
        "        # Criar ou esvaziar as subpastas para train, val, test\n",
        "        sets = ['train', 'train_labels', 'val', 'val_labels', 'test', 'test_labels']\n",
        "        \n",
        "        for set_type in sets:\n",
        "            dir_path = os.path.join(self.dir_new_dataset, set_type)\n",
        "            self.create_or_reset_directory(dir_path)\n",
        "\n",
        "        # Definir os caminhos originais dos datasets\n",
        "        dirs = {set_type: os.path.join(self.dir_dataset, set_type) for set_type in sets}\n",
        "        \n",
        "        # Carregar e processar as imagens de cada conjunto\n",
        "        for set_type in ['train', 'val', 'test']:\n",
        "            images_dir = dirs[set_type]\n",
        "            labels_dir = dirs[set_type + '_labels']\n",
        "            num_images, dim_images = self.load_images_and_labels(images_dir, labels_dir, set_type)\n",
        "            self.dim_images = dim_images\n",
        "            if set_type == 'train':\n",
        "                self.num_images_train = num_images\n",
        "            elif set_type == 'val':\n",
        "                self.num_images_val = num_images\n",
        "            else:\n",
        "                self.num_images_test = num_images\n",
        "            \n",
        "    def load_images_and_labels(self, images_dir, labels_dir, set_type):\n",
        "        # Diretórios onde as imagens e rótulos processados serão salvos\n",
        "        processed_dir = os.path.join(self.dir_new_dataset, set_type)\n",
        "        processed_label_dir = os.path.join(self.dir_new_dataset, f\"{set_type}_labels\")\n",
        "        count_images = 0   \n",
        "        \n",
        "        # Listar arquivos nos diretórios\n",
        "        image_files = sorted(os.listdir(images_dir))\n",
        "        label_files = sorted(os.listdir(labels_dir))\n",
        "        \n",
        "        prob_to_transform = self.prob_train if set_type == 'train' else self.prob_others\n",
        "        img_lbl_files = zip(image_files, label_files)\n",
        "        \n",
        "        for img_name, lbl_name in img_lbl_files:\n",
        "            transform_img = random.choices([True, False], weights=[prob_to_transform, 1 - prob_to_transform], k=1)[0]\n",
        "            \n",
        "            # Definir caminho para a imagem e rótulo\n",
        "            img_path = os.path.join(images_dir, img_name)\n",
        "            lbl_path = os.path.join(labels_dir, lbl_name)\n",
        "\n",
        "            # Carregar imagem e rótulo\n",
        "            img = cv2.imread(img_path)\n",
        "            lbl = cv2.imread(lbl_path)\n",
        "\n",
        "            # Reduzir a resolução da imagem\n",
        "            img = img[::self.partition, ::self.partition, :]\n",
        "            lbl = lbl[::self.partition, ::self.partition, :]\n",
        "\n",
        "            # Converter de BGR para RGB\n",
        "            img = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)\n",
        "            lbl = cv2.cvtColor(lbl, cv2.COLOR_BGR2RGB)\n",
        "\n",
        "            # Aplicar Gaussian blur\n",
        "            img = cv2.GaussianBlur(img, (3, 3), 0)\n",
        "\n",
        "            # Transformação aleatória\n",
        "            if transform_img:\n",
        "                img_transform = img.copy()\n",
        "                lbl_transform = lbl.copy()\n",
        "                img_transform, lbl_transform, trans = self.apply_random_transformation(img, lbl)\n",
        "\n",
        "                # Salvar imagem e rótulo transformado\n",
        "                count_images += 1\n",
        "                processed_img_path = os.path.join(processed_dir, f\"{img_name}_{trans}.png\")\n",
        "                processed_lbl_path = os.path.join(processed_label_dir, f\"{lbl_name}_{trans}.png\")\n",
        "                cv2.imwrite(processed_img_path, img_transform)\n",
        "                cv2.imwrite(processed_lbl_path, lbl_transform)\n",
        "\n",
        "                # Adicionar as imagens e rótulos transformados às listas\n",
        "                self.image_list.append(processed_img_path)\n",
        "                self.label_list.append(processed_lbl_path)\n",
        "                \n",
        "            # Salvar imagem e rótulo\n",
        "            count_images += 1\n",
        "            processed_img_path = os.path.join(processed_dir, img_name)\n",
        "            processed_lbl_path = os.path.join(processed_label_dir, lbl_name)\n",
        "\n",
        "            cv2.imwrite(processed_img_path, img)\n",
        "            cv2.imwrite(processed_lbl_path, lbl)\n",
        "\n",
        "            # Adicionar as imagens e rótulos processados às listas\n",
        "            self.image_list.append(processed_img_path)\n",
        "            self.label_list.append(processed_lbl_path)\n",
        "\n",
        "        return count_images, img.shape\n",
        "\n",
        "    def apply_random_transformation(self, img, lbl):\n",
        "        if self.with_brightness_contrast:\n",
        "            random_transform = random.randint(0, 9)\n",
        "        else:\n",
        "            random_transform = random.randint(0, 4)\n",
        "        if random_transform == 0:\n",
        "            arg_1, arg_2 = self.augment(img, lbl, \"rotation\")\n",
        "            return arg_1, arg_2, \"rotation\"\n",
        "        elif random_transform == 1:\n",
        "            arg_1, arg_2 = self.augment(img, lbl, \"flip\")\n",
        "            return arg_1, arg_2, \"flip\"\n",
        "        elif random_transform == 2:\n",
        "            arg_1, arg_2 = self.augment(img, lbl, \"flip_rotation\")\n",
        "            return arg_1, arg_2, \"flip_rotation\"\n",
        "        elif random_transform == 3:\n",
        "            arg_1, arg_2 = self.equalize_histogram(img, lbl)\n",
        "            return arg_1, arg_2, \"equalize_histogram\"\n",
        "        elif random_transform == 4:\n",
        "            arg_1, arg_2 = self.add_noise(img, lbl, noise_type=\"gaussian\")\n",
        "            return arg_1, arg_2, \"gaussian_noise\"\n",
        "        elif random_transform == 5:\n",
        "            arg_1, arg_2 = self.add_noise(img, lbl, noise_type=\"salt_pepper\")\n",
        "            return arg_1, arg_2, \"sal_pepper_noise\"\n",
        "        elif random_transform == 6:\n",
        "            arg_1, arg_2 = self.adjust_brightness_contrast(img, lbl, brightness=0, contrast=50)\n",
        "            return arg_1, arg_2, \"b_c_0_50\"\n",
        "        elif random_transform == 7:\n",
        "            arg_1, arg_2 = self.adjust_brightness_contrast(img, lbl, brightness=-10, contrast=30)\n",
        "            return arg_1, arg_2, \"b_c_-10_30\"\n",
        "        elif random_transform == 8:\n",
        "            arg_1, arg_2 = self.adjust_brightness_contrast(img, lbl, brightness=60, contrast=60)\n",
        "            return arg_1, arg_2, \"b_c_60_60\"\n",
        "        else:\n",
        "            arg_1, arg_2 = self.adjust_brightness_contrast(img, lbl, brightness=-20, contrast=0)\n",
        "            return arg_1, arg_2, \"b_c_-20_0\"\n",
        "\n",
        "    def rgb_to_label(self, mask):\n",
        "        unique_colors = np.unique(mask.reshape(-1, mask.shape[2]), axis=0)\n",
        "        label_mask = np.zeros((mask.shape[0], mask.shape[1]), dtype=np.int32)\n",
        "        for color in unique_colors:\n",
        "            color_tuple = tuple(color)\n",
        "            if color_tuple in self.rgb_to_index:\n",
        "                label = self.rgb_to_index[color_tuple]\n",
        "                matches = np.all(mask == color, axis=-1)\n",
        "                label_mask[matches] = label\n",
        "        return label_mask\n",
        "\n",
        "    def dict_labels(self, df_labels):\n",
        "        rgb_to_index = {}\n",
        "        index_to_label = {}\n",
        "        for count, row in df_labels.iterrows():\n",
        "            color = (row['r'], row['g'], row['b'])\n",
        "            label = row['name']\n",
        "            rgb_to_index[color] = count\n",
        "            index_to_label[count] = label\n",
        "        return rgb_to_index, index_to_label\n",
        "\n",
        "    def augment(self, img, lbl, transform):\n",
        "        if transform == \"rotation\":\n",
        "            rotation = cv2.getRotationMatrix2D((img.shape[1] // 2, img.shape[0] // 2), 180, 1)\n",
        "            rotated_image = cv2.warpAffine(img, rotation, (img.shape[1], img.shape[0]))\n",
        "            rotated_label = cv2.warpAffine(lbl, rotation, (lbl.shape[1], lbl.shape[0]), flags=cv2.INTER_NEAREST)\n",
        "            return rotated_image, rotated_label\n",
        "        elif transform == \"flip\":\n",
        "            flipped_image = cv2.flip(img, 1)\n",
        "            flipped_label = cv2.flip(lbl, 1)\n",
        "            return flipped_image, flipped_label\n",
        "        elif transform == \"flip_rotation\":\n",
        "            flipped_image = cv2.flip(img, 0)\n",
        "            flipped_label = cv2.flip(lbl, 0)\n",
        "            return flipped_image, flipped_label\n",
        "        \n",
        "   \n",
        "    def equalize_histogram(self, img, lbl):\n",
        "        img_yuv = cv2.cvtColor(img, cv2.COLOR_BGR2YUV)\n",
        "        img_yuv[:, :, 0] = cv2.equalizeHist(img_yuv[:, :, 0])\n",
        "        img = cv2.cvtColor(img_yuv, cv2.COLOR_YUV2BGR)\n",
        "        return img, lbl\n",
        "        \n",
        "    def adjust_brightness_contrast(self, img, lbl, brightness=0, contrast=0):\n",
        "        img = np.int16(img)\n",
        "        img = img * (contrast / 127 + 1) - contrast + brightness\n",
        "        img = np.clip(img, 0, 255)\n",
        "        img = np.uint8(img)\n",
        "        return img, lbl\n",
        "    \n",
        "    def add_noise(self, img, lbl, noise_type=\"gaussian\"):\n",
        "        if noise_type == \"gaussian\":\n",
        "            mean = 0\n",
        "            var = self.var_gaussian\n",
        "            sigma = var ** 0.5\n",
        "            gauss = np.random.normal(mean, sigma, img.shape)\n",
        "            noisy = img + gauss\n",
        "            noisy = np.clip(noisy, 0, 255)\n",
        "            return noisy, lbl\n",
        "    \n",
        "        elif noise_type == \"salt_pepper\":\n",
        "            s_vs_p = 0.5\n",
        "            amount = self.amount\n",
        "            out = np.copy(img)\n",
        "    \n",
        "            # Salt mode\n",
        "            num_salt = np.ceil(amount * img.size * s_vs_p)\n",
        "            coords = [np.random.randint(0, i, int(num_salt)) for i in img.shape]\n",
        "            out[tuple(coords)] = 255\n",
        "    \n",
        "            # Pepper mode\n",
        "            num_pepper = np.ceil(amount * img.size * (1.0 - s_vs_p))\n",
        "            coords = [np.random.randint(0, i, int(num_pepper)) for i in img.shape]\n",
        "            out[tuple(coords)] = 0\n",
        "    \n",
        "            return out, lbl"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 148,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Cuda is available:  True\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "<>:4: SyntaxWarning: invalid escape sequence '\\C'\n",
            "<>:4: SyntaxWarning: invalid escape sequence '\\C'\n",
            "C:\\Users\\Isas_\\AppData\\Local\\Temp\\ipykernel_17804\\1077995894.py:4: SyntaxWarning: invalid escape sequence '\\C'\n",
            "  dir_dataset = \"data\\CamVid\"\n"
          ]
        }
      ],
      "source": [
        "print(\"Cuda is available: \", torch.cuda.is_available())\n",
        "\n",
        "# Definindo os caminhos para cada conjunto\n",
        "dir_dataset = \"data\\CamVid\"\n",
        "\n",
        "class_dict = os.path.join(dir_dataset, 'class_dict.csv')\n",
        "df_labels = pd.read_csv(class_dict)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 149,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Número de imagens de treino:  536\n",
            "Número de imagens de validação:  111\n",
            "Número de imagens de teste:  264\n",
            "Dimensões das imagens:  (240, 320, 3)\n",
            "Número de classes:  32\n"
          ]
        }
      ],
      "source": [
        "# Instanciar a classe PreprocessDataset\n",
        "partition = 3  # Reduzir o tamanho da imagem por um fator de 3\n",
        "prob_train = 0.45  # Probabilidade de aplicar transformações no conjunto de treino\n",
        "prob_others = 0.15  # Probabilidade de aplicar transformações nos conjuntos de validação e teste\n",
        "dir_new_dataset = \"processed_data\" # Diretório onde os conjuntos processados serão salvos\n",
        "with_brightness_contrast = True # Aplicar transformações de brilho e contraste\n",
        "var_gaussian = 20 # Variância do ruído gaussiano\n",
        "amount = 0.02 # Quantidade de ruído sal e pimenta\n",
        "preprocessor = PreprocessDataset(dir_dataset, df_labels, dir_new_dataset, with_brightness_contrast, var_gaussian, amount, prob_train, prob_others, partition)\n",
        "\n",
        "# Carregar e processar as imagens e rótulos\n",
        "preprocessor.load_images_from_folder()\n",
        "\n",
        "# Número de imagens em cada conjunto\n",
        "print(\"Número de imagens de treino: \", preprocessor.num_images_train)\n",
        "print(\"Número de imagens de validação: \", preprocessor.num_images_val)\n",
        "print(\"Número de imagens de teste: \", preprocessor.num_images_test)\n",
        "print(\"Dimensões das imagens: \", preprocessor.dim_images)\n",
        "print(\"Número de classes: \", preprocessor.num_classes)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 150,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Número de imagens de treino:  561\n",
            "Número de imagens de validação:  119\n",
            "Número de imagens de teste:  274\n",
            "Dimensões das imagens:  (240, 320, 3)\n",
            "Número de classes:  32\n"
          ]
        }
      ],
      "source": [
        "# Instanciar a classe PreprocessDataset\n",
        "partition = 3  # Reduzir o tamanho da imagem por um fator de 3\n",
        "prob_train = 0.50  # Probabilidade de aplicar transformações no conjunto de treino\n",
        "prob_others = 0.20  # Probabilidade de aplicar transformações nos conjuntos de validação e teste\n",
        "dir_new_dataset = \"processed_data_0\" # Diretório onde os conjuntos processados serão salvos\n",
        "with_brightness_contrast = False # Não aplicar transformações de brilho e contraste\n",
        "var_gaussian = 15 # Variância do ruído gaussiano\n",
        "amount = 0.02 # Quantidade de ruído sal e pimenta\n",
        "preprocessor = PreprocessDataset(dir_dataset, df_labels, dir_new_dataset, with_brightness_contrast, var_gaussian, amount, prob_train, prob_others, partition)\n",
        "\n",
        "# Carregar e processar as imagens e rótulos\n",
        "preprocessor.load_images_from_folder()\n",
        "\n",
        "# Número de imagens em cada conjunto\n",
        "print(\"Número de imagens de treino: \", preprocessor.num_images_train)\n",
        "print(\"Número de imagens de validação: \", preprocessor.num_images_val)\n",
        "print(\"Número de imagens de teste: \", preprocessor.num_images_test)\n",
        "print(\"Dimensões das imagens: \", preprocessor.dim_images)\n",
        "print(\"Número de classes: \", preprocessor.num_classes)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Criando o DataLoader\n",
        "batch_size = 16  # Defina o tamanho do batch\n",
        "train_loader = DataLoader(\n",
        "    preprocessor,  # O dataset que criamos\n",
        "    batch_size=batch_size,  # Tamanho do batch\n",
        "    shuffle=True,  # Embaralha os dados a cada epoch\n",
        "    num_workers=4  # Quantos subprocessos usar para carregar os dados (ajuste para sua máquina)\n",
        ")\n",
        "\n",
        "# Exemplo de iteração no dataloader\n",
        "for images, labels in train_loader:\n",
        "    # Aqui você pode passar 'images' e 'labels' para o seu modelo\n",
        "    print(images.shape, labels.shape)\n",
        "    # Use as imagens e labels para treinar o seu modelo"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# (240, 320) block 1\n",
        "# (120, 160) block 2\n",
        "# (60, 80) block 3\n",
        "# (30, 40) block 4\n",
        "# (15, 20) block 5"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "class UNet(nn.Module):\n",
        "    def __init__(self, image_dim, n_channels=3, n_classes=32, depth=5, conv_kernel_size=3, conv_stride=1, conv_padding=1, pool_kernel_size=2, pool_stride=2, pool_padding=0,  transpose_kernel_size=3, transpose_stride=2, transpose_padding=1):\n",
        "        super(UNet, self).__init__()\n",
        "\n",
        "        self.image_dim = image_dim # Dimensões da imagem de entrada (C, H, W)\n",
        "        self.depth = depth\n",
        "        self.n_channels = n_channels\n",
        "        self.n_classes = n_classes\n",
        "        self.conv_kernel_size = conv_kernel_size\n",
        "        self.conv_stride = conv_stride\n",
        "        self.conv_padding = conv_padding\n",
        "        self.pool_kernel_size = pool_kernel_size\n",
        "        self.pool_stride = pool_stride\n",
        "        self.pool_padding = pool_padding\n",
        "        self.transpose_kernel_size = transpose_kernel_size\n",
        "        self.transpose_stride = transpose_stride\n",
        "        self.transpose_padding = transpose_padding\n",
        "        self.dim_encoders_block = {}\n",
        "        self.dim_decoders_block = {}\n",
        "        self.dim_bridge = None\n",
        "        self.dim_final_conv = None\n",
        "        self.crop_dict = {}\n",
        "\n",
        "    def calculate_dimensions(self):\n",
        "        h, w = self.image_dim[1], self.image_dim[2]  # Altura e largura da imagem de entrada\n",
        "        count = 1\n",
        "        # Calculando as dimensões ao longo do caminho de encoder (convoluções + pooling)\n",
        "        for _ in range(self.depth):\n",
        "            # Convolução\n",
        "            h = (h + 2 * self.conv_padding - self.conv_kernel_size) // self.conv_stride + 1\n",
        "            w = (w + 2 * self.conv_padding - self.conv_kernel_size) // self.conv_stride + 1\n",
        "            # Pooling\n",
        "            h = (h - self.pool_kernel_size) // self.pool_stride + 1\n",
        "            w = (w - self.pool_kernel_size) // self.pool_stride + 1\n",
        "\n",
        "            self.dim_encoders_block[f\"block_{count}\"] = (h, w)\n",
        "            \n",
        "            if h <= 2 or w <= 2:\n",
        "                print(f\"Encoder block {count} reached 2x2 dimension\")\n",
        "                print(f\"Encoder block {count} dimensions: {h}x{w}\")\n",
        "                print(f\"Stopping the calculation\")\n",
        "                break\n",
        "            count += 1\n",
        "        # Tamanho da camada de bottleneck (ponte entre encoder e decoder)\n",
        "        self.dim_bridge = (h, w)\n",
        "\n",
        "        # Calculando as dimensões ao longo do caminho de decoder (convoluções transpostas)\n",
        "        for _ in range(self.depth):\n",
        "            # Convolução transposta (upsampling)\n",
        "            h = (h - 1) * self.transpose_stride - 2 * self.transpose_padding + self.transpose_kernel_size\n",
        "            w = (w - 1) * self.transpose_stride - 2 * self.transpose_padding + self.transpose_kernel_size\n",
        "\n",
        "            self.dim_decoders_block[f\"block_{count}\"] = (h, w)\n",
        "            count -= 1  # Para espelhar os blocos de encoder e decoder\n",
        "\n",
        "        # Tamanho da camada de saída (última convolução)\n",
        "        self.dim_final_conv = (h, w)\n",
        "\n",
        "\n",
        "    def conv_block(self, in_channels, out_channels): \n",
        "        # Camada convolucional com normalização e função de ativação; 2 vezes\n",
        "        return nn.Sequential(\n",
        "            nn.Conv2d(in_channels, out_channels, kernel_size=self.conv_kernel_size, stride=self.conv_stride, padding=self.conv_padding), \n",
        "            nn.BatchNorm2d(out_channels), # Normalização para acelerar o treinamento\n",
        "            nn.ReLU(inplace=True), # Função de ativação (zera os valores negativos)\n",
        "            nn.Conv2d(out_channels, out_channels, kernel_size=self.conv_kernel_size, stride=self.conv_stride, padding=self.conv_padding),\n",
        "            nn.BatchNorm2d(out_channels),\n",
        "            nn.ReLU(inplace=True)\n",
        "        )\n",
        "    \n",
        "    def max_pool(self, ): # Função para reduzir a resolução da imagem\n",
        "        return nn.MaxPool2d(kernel_size=self.pool_kernel_size, stride=self.pool_stride, padding=self.pool_padding)\n",
        "    \n",
        "    def crop(features, target_size):\n",
        "        _, _, h, w = features.size()\n",
        "        target_h, target_w = target_size\n",
        "        delta_h = (h - target_h) // 2\n",
        "        delta_w = (w - target_w) // 2\n",
        "        return features[:, :, delta_h:delta_h + target_h, delta_w:delta_w + target_w]\n",
        "    \n",
        "    def encoder_block(self):\n",
        "        return nn.Sequential(\n",
        "            self.conv_block(),\n",
        "            self.max_pool()\n",
        "        )\n",
        "    \n",
        "    def decoder_block(self):\n",
        "        return nn.Sequential(\n",
        "            self.conv_transpose(),\n",
        "            self.conv_block()\n",
        "        )\n",
        "    \n",
        "    def conv_transpose(self):\n",
        "        pass\n",
        "    def final_conv(self, n_channels, n_classes):\n",
        "        return nn.Conv2d(n_channels, n_classes, kernel_size=1)\n",
        "\n",
        "    \n",
        "    def forward(self, x):\n",
        "        enc1 = self.enc1(x)\n",
        "        enc2 = self.enc2(enc1)\n",
        "        enc3 = self.enc3(enc2)\n",
        "        enc4 = self.enc4(enc3)\n",
        "        \n",
        "        bridge = self.bridge(enc4)\n",
        "        \n",
        "        dec1 = self.dec1(torch.cat([bridge, enc4], dim=1))\n",
        "        dec2 = self.dec2(torch.cat([dec1, enc3], dim=1))\n",
        "        dec3 = self.dec3(torch.cat([dec2, enc2], dim=1))\n",
        "        dec4 = self.dec4(torch.cat([dec3, enc1], dim=1))\n",
        "        \n",
        "        return self.final_conv(dec4)\n"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
