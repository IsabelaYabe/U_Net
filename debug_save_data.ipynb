{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import cv2\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from PIL import Image\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import random\n",
    "import torch\n",
    "import shutil"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class PreprocessDataset(Dataset):\n",
    "    def __init__(self, dir_dataset, df_labels, dir_new_dataset, set_type=\"train\", with_brightness_contrast=True, var_gaussian=10, amount = 0.02, prob_gen=0.4, partition=3): \n",
    "        self.with_brightness_contrast = with_brightness_contrast\n",
    "        self.var_gaussian = var_gaussian\n",
    "        self.amount = amount\n",
    "        self.dir_dataset = dir_dataset\n",
    "        self.prob_gen = prob_gen\n",
    "        self.set_type = set_type\n",
    "        self.dir_new_dataset = dir_new_dataset\n",
    "        self.partition = partition\n",
    "        self.rgb_to_index, self.index_to_label = self.dict_labels(df_labels)\n",
    "        self.df_labels = df_labels\n",
    "        self.dim_images = None\n",
    "        self.num_images = None\n",
    "        self.image_list = [] \n",
    "        self.label_list = [] \n",
    "        self.num_classes = len(self.df_labels)\n",
    "        self.load_images_from_folder()\n",
    "    def __len__(self):\n",
    "        return len(self.image_list)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        # Carregar a imagem e o rótulo baseado no índice `idx`\n",
    "        img_path = self.image_list[idx]\n",
    "        lbl_path = self.label_list[idx]\n",
    "\n",
    "        # Carregar a imagem e o rótulo do disco\n",
    "        img = cv2.imread(img_path)\n",
    "        lbl = cv2.imread(lbl_path)\n",
    "\n",
    "        # Normalizar a imagem (opcional)\n",
    "        img = img / 255.0\n",
    "\n",
    "        # Converter os dados para tensores\n",
    "        img = torch.tensor(img, dtype=torch.float32).permute(2, 0, 1)  # De (H, W, C) para (C, H, W)\n",
    "        lbl = torch.tensor(lbl, dtype=torch.long).permute(2, 0, 1)  # De (H, W, C) para (C, H, W)\n",
    "\n",
    "        return img, lbl\n",
    "\n",
    "    def create_or_reset_directory(self):\n",
    "        new_dir_img = os.path.join(self.dir_new_dataset, self.set_type)\n",
    "        new_dir_lbl = os.path.join(self.dir_new_dataset, f\"{self.set_type}_labels\")\n",
    "        \n",
    "        # Verificar se o diretório de imagens já existe; se não, criar\n",
    "        if not os.path.exists(new_dir_img):\n",
    "            os.makedirs(new_dir_img)  # Cria o diretório se não existir\n",
    "        \n",
    "        # Verificar se o diretório de rótulos já existe; se não, criar\n",
    "        if not os.path.exists(new_dir_lbl):\n",
    "            os.makedirs(new_dir_lbl)  # Cria o diretório se não existir\n",
    "        \n",
    "    def load_images_from_folder(self):\n",
    "        # Definir os caminhos originais dos datasets\n",
    "        images_dir = os.path.join(self.dir_dataset, self.set_type)\n",
    "        labels_dir = os.path.join(self.dir_dataset, f\"{self.set_type}_labels\")\n",
    "        \n",
    "        dim_images = self.load_images_and_labels(images_dir, labels_dir)\n",
    "        self.dim_images = dim_images\n",
    "            \n",
    "    def load_images_and_labels(self, images_dir, labels_dir):\n",
    "        self.create_or_reset_directory()\n",
    "\n",
    "        # Listar arquivos nos diretórios\n",
    "        image_files = sorted(os.listdir(images_dir))\n",
    "        label_files = sorted(os.listdir(labels_dir))\n",
    "\n",
    "        prob_to_transform = self.prob_gen\n",
    "        img_lbl_files = zip(image_files, label_files)\n",
    "\n",
    "        for img_name, lbl_name in img_lbl_files:\n",
    "            transform_img = random.choices([True, False], weights=[prob_to_transform, 1 - prob_to_transform], k=1)[0]\n",
    "\n",
    "            # Definir caminho para a imagem e rótulo\n",
    "            img_path = os.path.join(images_dir, img_name)\n",
    "            lbl_path = os.path.join(labels_dir, lbl_name)\n",
    "\n",
    "            # Carregar imagem e rótulo\n",
    "            img = cv2.imread(img_path)\n",
    "            lbl = cv2.imread(lbl_path)\n",
    "\n",
    "            # Reduzir a resolução da imagem\n",
    "            img = img[::self.partition, ::self.partition, :]\n",
    "            lbl = lbl[::self.partition, ::self.partition, :]\n",
    "\n",
    "            # Converter de BGR para RGB\n",
    "            img = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)\n",
    "            lbl = cv2.cvtColor(lbl, cv2.COLOR_BGR2RGB)\n",
    "\n",
    "            # Aplicar Gaussian blur\n",
    "            img = cv2.GaussianBlur(img, (3, 3), 0)\n",
    "\n",
    "            # Transformação aleatória\n",
    "            if transform_img:\n",
    "                img_transform = img.copy()\n",
    "                lbl_transform = lbl.copy()\n",
    "                img_transform, lbl_transform, trans = self.apply_random_transformation(img_transform, lbl_transform)\n",
    "\n",
    "                # Salvar imagem e rótulo transformado com sufixo de transformação\n",
    "                transformed_img_name = f\"{img_name}_{trans}.png\"\n",
    "                transformed_lbl_name = f\"{lbl_name}_{trans}.png\"\n",
    "\n",
    "                processed_img_path = os.path.join(self.dir_new_dataset, self.set_type, transformed_img_name)\n",
    "                processed_lbl_path = os.path.join(self.dir_new_dataset, f\"{self.set_type}_labels\", transformed_lbl_name)\n",
    "\n",
    "                # Verifique se o arquivo já existe antes de salvar\n",
    "                if not os.path.exists(processed_img_path):\n",
    "                    cv2.imwrite(processed_img_path, img_transform)\n",
    "                    cv2.imwrite(processed_lbl_path, lbl_transform)\n",
    "\n",
    "                    # Adicionar as imagens e rótulos transformados às listas\n",
    "                    self.image_list.append(processed_img_path)\n",
    "                    self.label_list.append(processed_lbl_path)\n",
    "\n",
    "            # Criar caminho para salvar a imagem transformada\n",
    "            processed_img_path = os.path.join(self.dir_new_dataset, self.set_type, img_name)\n",
    "            processed_lbl_path = os.path.join(self.dir_new_dataset, f\"{self.set_type}_labels\", lbl_name)\n",
    "\n",
    "            cv2.imwrite(processed_img_path, img)\n",
    "            cv2.imwrite(processed_lbl_path, lbl)\n",
    "            \n",
    "            # Adicionar as imagens e rótulos processados às listas\n",
    "            self.image_list.append(processed_img_path)\n",
    "            self.label_list.append(processed_lbl_path)\n",
    "\n",
    "        return img.shape\n",
    "\n",
    "    def apply_random_transformation(self, img, lbl):\n",
    "        if self.with_brightness_contrast:\n",
    "            random_transform = random.randint(0, 9)\n",
    "        else:\n",
    "            random_transform = random.randint(0, 4)\n",
    "        if random_transform == 0:\n",
    "            arg_1, arg_2 = self.augment(img, lbl, \"rotation\")\n",
    "            return arg_1, arg_2, \"rotation\"\n",
    "        elif random_transform == 1:\n",
    "            arg_1, arg_2 = self.augment(img, lbl, \"flip\")\n",
    "            return arg_1, arg_2, \"flip\"\n",
    "        elif random_transform == 2:\n",
    "            arg_1, arg_2 = self.augment(img, lbl, \"flip_rotation\")\n",
    "            return arg_1, arg_2, \"flip_rotation\"\n",
    "        elif random_transform == 3:\n",
    "            arg_1, arg_2 = self.equalize_histogram(img, lbl)\n",
    "            return arg_1, arg_2, \"equalize_histogram\"\n",
    "        elif random_transform == 4:\n",
    "            arg_1, arg_2 = self.add_noise(img, lbl, noise_type=\"gaussian\")\n",
    "            return arg_1, arg_2, \"gaussian_noise\"\n",
    "        elif random_transform == 5:\n",
    "            arg_1, arg_2 = self.add_noise(img, lbl, noise_type=\"salt_pepper\")\n",
    "            return arg_1, arg_2, \"sal_pepper_noise\"\n",
    "        elif random_transform == 6:\n",
    "            arg_1, arg_2 = self.adjust_brightness_contrast(img, lbl, brightness=0, contrast=50)\n",
    "            return arg_1, arg_2, \"b_c_0_50\"\n",
    "        elif random_transform == 7:\n",
    "            arg_1, arg_2 = self.adjust_brightness_contrast(img, lbl, brightness=-10, contrast=30)\n",
    "            return arg_1, arg_2, \"b_c_-10_30\"\n",
    "        elif random_transform == 8:\n",
    "            arg_1, arg_2 = self.adjust_brightness_contrast(img, lbl, brightness=60, contrast=60)\n",
    "            return arg_1, arg_2, \"b_c_60_60\"\n",
    "        else:\n",
    "            arg_1, arg_2 = self.adjust_brightness_contrast(img, lbl, brightness=-20, contrast=0)\n",
    "            return arg_1, arg_2, \"b_c_-20_0\"\n",
    "\n",
    "    def rgb_to_label(self, mask):\n",
    "        unique_colors = np.unique(mask.reshape(-1, mask.shape[2]), axis=0)\n",
    "        label_mask = np.zeros((mask.shape[0], mask.shape[1]), dtype=np.int32)\n",
    "        for color in unique_colors:\n",
    "            color_tuple = tuple(color)\n",
    "            if color_tuple in self.rgb_to_index:\n",
    "                label = self.rgb_to_index[color_tuple]\n",
    "                matches = np.all(mask == color, axis=-1)\n",
    "                label_mask[matches] = label\n",
    "        return label_mask\n",
    "\n",
    "    def dict_labels(self, df_labels):\n",
    "        rgb_to_index = {}\n",
    "        index_to_label = {}\n",
    "        for count, row in df_labels.iterrows():\n",
    "            color = (row['r'], row['g'], row['b'])\n",
    "            label = row['name']\n",
    "            rgb_to_index[color] = count\n",
    "            index_to_label[count] = label\n",
    "        return rgb_to_index, index_to_label\n",
    "\n",
    "    def augment(self, img, lbl, transform):\n",
    "        if transform == \"rotation\":\n",
    "            rotation = cv2.getRotationMatrix2D((img.shape[1] // 2, img.shape[0] // 2), 180, 1)\n",
    "            rotated_image = cv2.warpAffine(img, rotation, (img.shape[1], img.shape[0]))\n",
    "            rotated_label = cv2.warpAffine(lbl, rotation, (lbl.shape[1], lbl.shape[0]), flags=cv2.INTER_NEAREST)\n",
    "            return rotated_image, rotated_label\n",
    "        elif transform == \"flip\":\n",
    "            flipped_image = cv2.flip(img, 1)\n",
    "            flipped_label = cv2.flip(lbl, 1)\n",
    "            return flipped_image, flipped_label\n",
    "        elif transform == \"flip_rotation\":\n",
    "            flipped_image = cv2.flip(img, 0)\n",
    "            flipped_label = cv2.flip(lbl, 0)\n",
    "            return flipped_image, flipped_label\n",
    "        \n",
    "   \n",
    "    def equalize_histogram(self, img, lbl):\n",
    "        img_yuv = cv2.cvtColor(img, cv2.COLOR_BGR2YUV)\n",
    "        img_yuv[:, :, 0] = cv2.equalizeHist(img_yuv[:, :, 0])\n",
    "        img = cv2.cvtColor(img_yuv, cv2.COLOR_YUV2BGR)\n",
    "        return img, lbl\n",
    "        \n",
    "    def adjust_brightness_contrast(self, img, lbl, brightness=0, contrast=0):\n",
    "        img = np.int16(img)\n",
    "        img = img * (contrast / 127 + 1) - contrast + brightness\n",
    "        img = np.clip(img, 0, 255)\n",
    "        img = np.uint8(img)\n",
    "        return img, lbl\n",
    "    \n",
    "    def add_noise(self, img, lbl, noise_type=\"gaussian\"):\n",
    "        if noise_type == \"gaussian\":\n",
    "            mean = 0\n",
    "            var = self.var_gaussian\n",
    "            sigma = var ** 0.5\n",
    "            gauss = np.random.normal(mean, sigma, img.shape)\n",
    "            noisy = img + gauss\n",
    "            noisy = np.clip(noisy, 0, 255)\n",
    "            return noisy, lbl\n",
    "    \n",
    "        elif noise_type == \"salt_pepper\":\n",
    "            s_vs_p = 0.5\n",
    "            amount = self.amount\n",
    "            out = np.copy(img)\n",
    "    \n",
    "            # Salt mode\n",
    "            num_salt = np.ceil(amount * img.size * s_vs_p)\n",
    "            coords = [np.random.randint(0, i, int(num_salt)) for i in img.shape]\n",
    "            out[tuple(coords)] = 255\n",
    "    \n",
    "            # Pepper mode\n",
    "            num_pepper = np.ceil(amount * img.size * (1.0 - s_vs_p))\n",
    "            coords = [np.random.randint(0, i, int(num_pepper)) for i in img.shape]\n",
    "            out[tuple(coords)] = 0\n",
    "    \n",
    "            return out, lbl"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "class UNet(nn.Module):\n",
    "    def __init__(self, image_dim, n_channels=64, n_classes=32, depth=5, conv_kernel_size=3, conv_stride=1, conv_padding=1, pool_kernel_size=2, pool_stride=2, pool_padding=0, transpose_kernel_size=3, transpose_stride=2, transpose_padding=1):\n",
    "        super(UNet, self).__init__()\n",
    "\n",
    "        self.image_dim = image_dim  # Dimensões da imagem de entrada (C, H, W)\n",
    "        self.depth = depth \n",
    "        self.n_channels = n_channels\n",
    "        self.n_classes = n_classes\n",
    "        self.conv_kernel_size = conv_kernel_size\n",
    "        self.conv_stride = conv_stride\n",
    "        self.conv_padding = conv_padding\n",
    "        self.pool_kernel_size = pool_kernel_size\n",
    "        self.pool_stride = pool_stride\n",
    "        self.pool_padding = pool_padding\n",
    "        self.transpose_kernel_size = transpose_kernel_size\n",
    "        self.transpose_stride = transpose_stride\n",
    "        self.transpose_padding = transpose_padding\n",
    "\n",
    "        # Encoder\n",
    "        self.encoders = nn.ModuleList([self.conv_block(3 if i == 0 else self.n_channels * (2 ** (i-1)), self.n_channels * (2 ** i)) for i in range(self.depth)])\n",
    "        self.pool = nn.MaxPool2d(kernel_size=self.pool_kernel_size, stride=self.pool_stride, padding=self.pool_padding)\n",
    "\n",
    "        # Bottleneck\n",
    "        self.bottleneck = self.conv_block(self.n_channels * (2 ** (self.depth-1)), self.n_channels * (2 ** self.depth))\n",
    "\n",
    "        # Decoder\n",
    "        self.decoders = nn.ModuleList([self.conv_transpose(self.n_channels * (2 ** (i+2)), self.n_channels * (2 ** i)) for i in range(self.depth-2, -1, -1)])\n",
    "\n",
    "        # Final conv layer\n",
    "        self.final_conv = nn.Conv2d(self.n_channels, n_classes, kernel_size=1)\n",
    "\n",
    "    def conv_block(self, in_channels, out_channels):\n",
    "        # Camada convolucional com normalização e função de ativação; 2 vezes\n",
    "        return nn.Sequential(\n",
    "            nn.Conv2d(in_channels, out_channels, kernel_size=self.conv_kernel_size, stride=self.conv_stride, padding=self.conv_padding),\n",
    "            nn.BatchNorm2d(out_channels),  # Normalização para acelerar o treinamento\n",
    "            nn.ReLU(inplace=True),  # Função de ativação (zera os valores negativos)\n",
    "            nn.Conv2d(out_channels, out_channels, kernel_size=self.conv_kernel_size, stride=self.conv_stride, padding=self.conv_padding),\n",
    "            nn.BatchNorm2d(out_channels),\n",
    "            nn.ReLU(inplace=True)\n",
    "        )\n",
    "\n",
    "    def crop(self, encoder_feature, decoder_feature):\n",
    "        _, _, h, w = decoder_feature.size()\n",
    "        encoder_feature = F.interpolate(encoder_feature, size=(h, w), mode='bilinear', align_corners=False)  # Redimensiona a feature map do encoder\n",
    "        return encoder_feature\n",
    "\n",
    "    def conv_transpose(self, in_channels, out_channels):\n",
    "        return nn.Sequential(\n",
    "            nn.ConvTranspose2d(in_channels, out_channels, kernel_size=self.transpose_kernel_size, stride=self.transpose_stride, padding=self.transpose_padding),\n",
    "            self.conv_block(out_channels, out_channels)\n",
    "        )\n",
    "    \n",
    "    def forward(self, x):\n",
    "        print(f\"Input shape: {x.shape}\")\n",
    "        encoders_features = []\n",
    "\n",
    "        # Encoder pass\n",
    "        for idx, encoder in enumerate(self.encoders):\n",
    "            print(\"idx: \", idx)\n",
    "            print(\"encoder: \", encoder)\n",
    "            x = encoder(x)\n",
    "            encoders_features.append(x)\n",
    "            print(f\"After encoder block {idx+1}: {x.shape}\")\n",
    "            x = self.pool(x)\n",
    "            print(f\"After pooling {idx+1}: {x.shape}\")\n",
    "\n",
    "        # Bottleneck\n",
    "        x = self.bottleneck(x)\n",
    "        print(f\"After bottleneck: {x.shape}\")\n",
    "        # Doubled the block\n",
    "        print(\"Starting decoder pass\")\n",
    "        # Decoder pass\n",
    "        for i, decoder in enumerate(self.decoders):\n",
    "            print(\"i: \", i)\n",
    "            print(\"decoder: \", decoder)\n",
    "            encoder_feature = encoders_features[-(i+1)]\n",
    "            encoder_feature = self.crop(encoder_feature, x)  # Aplica o crop nas feature maps\n",
    "            print(f\"Encoder feature {i+1} after crop: {encoder_feature.shape}\")\n",
    "\n",
    "            if i != 0:\n",
    "                x = torch.cat([encoder_feature, x], dim=1)  # Concatena encoder com decoder\n",
    "                print(f\"After concatenation with encoder feature {i+1}: {x.shape}\")\n",
    "\n",
    "            x = decoder(x)\n",
    "\n",
    "        # Final convolution\n",
    "        x = self.final_conv(x)\n",
    "        print(f\"Output shape after final convolution: {x.shape}\")\n",
    "\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<>:2: SyntaxWarning: invalid escape sequence '\\C'\n",
      "<>:2: SyntaxWarning: invalid escape sequence '\\C'\n",
      "C:\\Users\\Isas_\\AppData\\Local\\Temp\\ipykernel_27584\\1657100305.py:2: SyntaxWarning: invalid escape sequence '\\C'\n",
      "  dir_dataset = \"data\\CamVid\"\n"
     ]
    }
   ],
   "source": [
    "# Definindo os caminhos para cada conjunto\n",
    "dir_dataset = \"data\\CamVid\"\n",
    "\n",
    "class_dict = os.path.join(dir_dataset, 'class_dict.csv')\n",
    "df_labels = pd.read_csv(class_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Função para remover todos subdiretórios e arquivos de um diretório\n",
    "def remove_all_files_from_dir(dir_path):\n",
    "    for item in os.listdir(dir_path):\n",
    "        item_path = os.path.join(dir_path, item)\n",
    "        if os.path.isfile(item_path):\n",
    "            os.remove(item_path)\n",
    "        elif os.path.isdir(item_path):\n",
    "            shutil.rmtree(item_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Número de batches de treino: 22\n",
      "Número de batches de validação: 6\n",
      "Número de batches de teste: 14\n",
      "Batch de imagens: torch.Size([24, 3, 240, 320])\n",
      "Batch de labels: torch.Size([24, 3, 240, 320])\n"
     ]
    }
   ],
   "source": [
    "remove_all_files_from_dir(\"debug_processed_data\")\n",
    "# Parâmetros\n",
    "partition = 3  # Reduzir o tamanho da imagem por um fator de 4\n",
    "prob_train = 0.25  # Probabilidade de aplicar transformações no conjunto de treino\n",
    "prob_others = 0.10  # Probabilidade de aplicar transformações nos conjuntos de validação e teste\n",
    "dir_new_dataset = \"debug_processed_data\"  # Diretório onde os conjuntos processados serão salvos\n",
    "with_brightness_contrast = True  # Aplicar transformações de brilho e contraste\n",
    "var_gaussian = 20  # Variância do ruído gaussiano\n",
    "amount = 0.02  # Quantidade de ruído sal e pimenta\n",
    "\n",
    "# Criando o dataset para o conjunto de treino\n",
    "train_dataset = PreprocessDataset(\n",
    "    dir_dataset=dir_dataset,\n",
    "    df_labels=df_labels,\n",
    "    dir_new_dataset=dir_new_dataset,\n",
    "    set_type='train',  # Carrega o conjunto de treino\n",
    "    partition=partition  # Reduzir a imagem por um fator de 4\n",
    ")\n",
    "\n",
    "# Criando o dataset para o conjunto de validação\n",
    "val_dataset = PreprocessDataset(\n",
    "    dir_dataset=dir_dataset,\n",
    "    df_labels=df_labels,\n",
    "    dir_new_dataset=dir_new_dataset,\n",
    "    set_type='val',  # Carrega o conjunto de validação\n",
    "    partition=partition\n",
    ")\n",
    "\n",
    "# Criando o dataset para o conjunto de teste\n",
    "test_dataset = PreprocessDataset(\n",
    "    dir_dataset=dir_dataset,\n",
    "    df_labels=df_labels,\n",
    "    dir_new_dataset=dir_new_dataset,\n",
    "    set_type='test',  # Carrega o conjunto de teste\n",
    "    partition=partition\n",
    ")\n",
    "\n",
    "# Definindo o tamanho do batch\n",
    "batch_size = 24\n",
    "\n",
    "# Criando o DataLoader para o conjunto de treino\n",
    "train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "\n",
    "# Criando o DataLoader para o conjunto de validação\n",
    "val_loader = DataLoader(val_dataset, batch_size=batch_size, shuffle=False)\n",
    "\n",
    "# Criando o DataLoader para o conjunto de teste\n",
    "test_loader = DataLoader(test_dataset, batch_size=batch_size, shuffle=False)\n",
    "\n",
    "print(f'Número de batches de treino: {len(train_loader)}')\n",
    "print(f'Número de batches de validação: {len(val_loader)}')\n",
    "print(f'Número de batches de teste: {len(test_loader)}')\n",
    "\n",
    "# Iterando sobre o DataLoader de treino\n",
    "for images, labels in train_loader:\n",
    "    print(f'Batch de imagens: {images.shape}')\n",
    "    print(f'Batch de labels: {labels.shape}')\n",
    "    # Aqui você pode passar as imagens e labels para o seu modelo\n",
    "    break  # Apenas um exemplo, interrompendo após o primeiro batch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dispositivo: cuda\n",
      "Imagens e rótulos movidos para o dispositivo.\n",
      "torch.Size([24, 3, 240, 320])\n",
      "torch.Size([24, 3, 240, 320])\n",
      "Iniciando forward pass...\n",
      "Input shape: torch.Size([24, 3, 240, 320])\n",
      "idx:  0\n",
      "encoder:  Sequential(\n",
      "  (0): Conv2d(3, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "  (1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "  (2): ReLU(inplace=True)\n",
      "  (3): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "  (4): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "  (5): ReLU(inplace=True)\n",
      ")\n",
      "After encoder block 1: torch.Size([24, 64, 240, 320])\n",
      "After pooling 1: torch.Size([24, 64, 120, 160])\n",
      "idx:  1\n",
      "encoder:  Sequential(\n",
      "  (0): Conv2d(64, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "  (1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "  (2): ReLU(inplace=True)\n",
      "  (3): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "  (4): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "  (5): ReLU(inplace=True)\n",
      ")\n",
      "After encoder block 2: torch.Size([24, 128, 120, 160])\n",
      "After pooling 2: torch.Size([24, 128, 60, 80])\n",
      "idx:  2\n",
      "encoder:  Sequential(\n",
      "  (0): Conv2d(128, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "  (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "  (2): ReLU(inplace=True)\n",
      "  (3): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "  (4): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "  (5): ReLU(inplace=True)\n",
      ")\n",
      "After encoder block 3: torch.Size([24, 256, 60, 80])\n",
      "After pooling 3: torch.Size([24, 256, 30, 40])\n",
      "idx:  3\n",
      "encoder:  Sequential(\n",
      "  (0): Conv2d(256, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "  (1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "  (2): ReLU(inplace=True)\n",
      "  (3): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "  (4): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "  (5): ReLU(inplace=True)\n",
      ")\n",
      "After encoder block 4: torch.Size([24, 512, 30, 40])\n",
      "After pooling 4: torch.Size([24, 512, 15, 20])\n",
      "idx:  4\n",
      "encoder:  Sequential(\n",
      "  (0): Conv2d(512, 1024, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "  (1): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "  (2): ReLU(inplace=True)\n",
      "  (3): Conv2d(1024, 1024, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "  (4): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "  (5): ReLU(inplace=True)\n",
      ")\n",
      "After encoder block 5: torch.Size([24, 1024, 15, 20])\n",
      "After pooling 5: torch.Size([24, 1024, 7, 10])\n",
      "After bottleneck: torch.Size([24, 2048, 7, 10])\n",
      "Starting decoder pass\n",
      "i:  0\n",
      "decoder:  Sequential(\n",
      "  (0): ConvTranspose2d(2048, 512, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1))\n",
      "  (1): Sequential(\n",
      "    (0): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (2): ReLU(inplace=True)\n",
      "    (3): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (4): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (5): ReLU(inplace=True)\n",
      "  )\n",
      ")\n",
      "Encoder feature 1 after crop: torch.Size([24, 1024, 7, 10])\n",
      "i:  1\n",
      "decoder:  Sequential(\n",
      "  (0): ConvTranspose2d(1024, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1))\n",
      "  (1): Sequential(\n",
      "    (0): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (2): ReLU(inplace=True)\n",
      "    (3): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (4): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (5): ReLU(inplace=True)\n",
      "  )\n",
      ")\n",
      "Encoder feature 2 after crop: torch.Size([24, 512, 13, 19])\n",
      "After concatenation with encoder feature 2: torch.Size([24, 1024, 13, 19])\n",
      "i:  2\n",
      "decoder:  Sequential(\n",
      "  (0): ConvTranspose2d(512, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1))\n",
      "  (1): Sequential(\n",
      "    (0): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (2): ReLU(inplace=True)\n",
      "    (3): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (4): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (5): ReLU(inplace=True)\n",
      "  )\n",
      ")\n",
      "Encoder feature 3 after crop: torch.Size([24, 256, 25, 37])\n",
      "After concatenation with encoder feature 3: torch.Size([24, 512, 25, 37])\n",
      "i:  3\n",
      "decoder:  Sequential(\n",
      "  (0): ConvTranspose2d(256, 64, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1))\n",
      "  (1): Sequential(\n",
      "    (0): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (2): ReLU(inplace=True)\n",
      "    (3): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (4): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (5): ReLU(inplace=True)\n",
      "  )\n",
      ")\n",
      "Encoder feature 4 after crop: torch.Size([24, 128, 49, 73])\n",
      "After concatenation with encoder feature 4: torch.Size([24, 256, 49, 73])\n",
      "Output shape after final convolution: torch.Size([24, 32, 97, 145])\n",
      "Imagens: torch.Size([24, 3, 240, 320])\n",
      "Labels: torch.Size([24, 3, 240, 320])\n",
      "Saída do modelo: torch.Size([24, 32, 97, 145])\n"
     ]
    }
   ],
   "source": [
    "# Instanciando o modelo U-Net com as dimensões das imagens\n",
    "model = UNet(image_dim=(3, 240, 320), n_channels=64, n_classes=32, depth=5, conv_kernel_size=3, conv_stride=1, conv_padding=1, pool_kernel_size=2, pool_stride=2, pool_padding=0, transpose_kernel_size=3, transpose_stride=2, transpose_padding=1)\n",
    "# Configurando o dispositivo (GPU, se disponível)\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print(f\"Dispositivo: {device}\")\n",
    "model = model.to(device)  # Mover o modelo para o dispositivo\n",
    "\n",
    "# Testando com um batch de dados do DataLoader\n",
    "for images, labels in train_loader:\n",
    "    # Move os dados para o dispositivo (GPU/CPU)\n",
    "    images = images.to(device)\n",
    "    labels = labels.to(device)\n",
    "    print(\"Imagens e rótulos movidos para o dispositivo.\")\n",
    "    print(images.shape)\n",
    "    print(labels.shape)\n",
    "    print(\"Iniciando forward pass...\")\n",
    "    # Passa as imagens pelo modelo U-Net\n",
    "    output = model(images)\n",
    "    \n",
    "    # Exibe as dimensões das imagens, labels e da saída do modelo\n",
    "    print(f\"Imagens: {images.shape}\")\n",
    "    print(f\"Labels: {labels.shape}\")\n",
    "    print(f\"Saída do modelo: {output.shape}\")\n",
    "    \n",
    "    # Quebrar após o primeiro batch, apenas para teste\n",
    "    break"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
